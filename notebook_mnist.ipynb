{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "4b7874d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "676bb996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a transform to convert images to tensors\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# Download the MNIST training dataset\n",
    "train_data = datasets.MNIST(\n",
    "    root='data',       # where to save the data\n",
    "    train=True,        # training set\n",
    "    download=True,     # download if not already present\n",
    "    transform=transform  # apply transformation\n",
    ")\n",
    "\n",
    "# Download the MNIST test dataset\n",
    "test_data = datasets.MNIST(\n",
    "    root='data',\n",
    "    train=False,       # test set\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "# If the output when running this block is 100%, the dataset was downloaded and stored correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "8b3728dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get one sample and print its label, then show the image using matplotlib\n",
    "image_tensor, label = train_data[0]  # get the first sample\n",
    "print(label)  # print the label\n",
    "\n",
    "# Visualize the first image in the training set\n",
    "image_tensor = train_data[0][0]\n",
    "image_to_plot = image_tensor.squeeze()\n",
    "plt.imshow(image_to_plot, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4d586b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple neural network for digit classification\n",
    "class CNNclassifier(nn.Module):  # 1. Inherit from nn.Module\n",
    "    def __init__(self):\n",
    "        super().__init__()  # 2. Init the base class\n",
    "        self.conv1 = nn.Conv2d(1,8,(3,3))\n",
    "        self.pool1 = nn.MaxPool2d((2,2)) \n",
    "        self.conv2 = nn.Conv2d(8,16,(3,3))\n",
    "        self.pool2 = nn.MaxPool2d((2,2)) \n",
    "        self.fc1 = nn.Linear(400, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "         # Hidden layer: 784 input features (28x28 pixels) to 128\n",
    "         # Output layer: 128 features to 10 classes (digits 0-9)\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112dd294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataLoader for batching the training data\n",
    "train_dataloader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "\n",
    "# Instantiate the model\n",
    "instance = CNNclassifier()\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.SGD(instance.parameters(), lr=0.02)\n",
    "\n",
    "# List to store loss values\n",
    "losses = []\n",
    "\n",
    "# Training loop (one epoch)\n",
    "for images, labels in train_dataloader:\n",
    "    # Forward pass: compute model output\n",
    "    model = instance(images)\n",
    "    # Define the loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    loss = criterion(model, labels)\n",
    "    losses.append(loss.item())  # Store loss value\n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ee752a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss curve\n",
    "#plt.figure(figsize=(8, 4))\n",
    "#plt.plot(losses, label='Training Loss')\n",
    "#plt.xlabel('Batch')\n",
    "#plt.ylabel('Loss')\n",
    "#plt.title('Training Loss Curve')\n",
    "#plt.legend()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba91ac02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90.62\n"
     ]
    }
   ],
   "source": [
    "# Create a DataLoader for batching the test data\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=False)\n",
    "correct = 0\n",
    "total = 0\n",
    "# Disable gradient calculation for evaluation\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_dataloader:\n",
    "        outputs = instance(images)  # Get model predictions\n",
    "        _, predicted = torch.max(outputs.data, 1)  # Get predicted class\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        # Print and visualize incorrect predictions\n",
    "        #for i in range(len(labels)):\n",
    "          #  if predicted[i] != labels[i]:\n",
    "           #     print(f\"Predicted: {predicted[i].item()}, Actual: {labels[i].item()}\")\n",
    "            #    image_tensor = images[i][0]\n",
    "             #   image_to_plot = image_tensor.squeeze()\n",
    "              #  plt.imshow(image_to_plot, cmap='gray')\n",
    "               # plt.axis('off')\n",
    "                #plt.show()\n",
    "# Calculate and print accuracy\n",
    "accuracy = 100 * correct / total\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f6bcd55a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 8, 24, 24]             208\n",
      "         MaxPool2d-2            [-1, 8, 12, 12]               0\n",
      "            Conv2d-3           [-1, 16, 10, 10]           1,168\n",
      "         MaxPool2d-4             [-1, 16, 5, 5]               0\n",
      "            Linear-5                  [-1, 128]          51,328\n",
      "            Linear-6                   [-1, 10]           1,290\n",
      "================================================================\n",
      "Total params: 53,994\n",
      "Trainable params: 53,994\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.06\n",
      "Params size (MB): 0.21\n",
      "Estimated Total Size (MB): 0.27\n",
      "----------------------------------------------------------------\n",
      "Model graph saved as cnn_classifier_graph.png\n"
     ]
    }
   ],
   "source": [
    "# Visualize the model summary and architecture\n",
    "from torchsummary import summary\n",
    "from torchviz import make_dot\n",
    "\n",
    "# Print a summary of the model (input shape: 1x28x28 for MNIST)\n",
    "summary(instance, (1, 28, 28))\n",
    "\n",
    "# Visualize the computation graph for a single batch\n",
    "sample_input = torch.randn(1, 1, 28, 28)\n",
    "output = instance(sample_input)\n",
    "dot = make_dot(output, params=dict(list(instance.named_parameters())))\n",
    "dot.format = 'png'\n",
    "dot.render('cnn_classifier_graph', view=False)  # Saves as cnn_classifier_graph.png in the current directory\n",
    "print('Model graph saved as cnn_classifier_graph.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
